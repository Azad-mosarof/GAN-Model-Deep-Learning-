{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-06 15:37:06.871097: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-06 15:37:06.871155: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from calendar import c\n",
    "from pickletools import optimize\n",
    "from pyexpat import model\n",
    "from statistics import mode\n",
    "from turtle import shape, update\n",
    "from unicodedata import name\n",
    "import keras \n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.layers import Input,Dense,Reshape,Flatten,Activation,MaxPool2D\n",
    "from keras.layers import BatchNormalization,Conv2DTranspose,Conv2D,Dropout\n",
    "from keras.layers import Input, ReLU\n",
    "from keras.models import Sequential,Model\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import LeakyReLU\n",
    "import cv2\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 128\n",
    "img_cols = 128\n",
    "img_channel = 3\n",
    "img_shape = (img_rows,img_cols,img_channel)\n",
    "img_dir = '/home/azadm/Desktop/Datasetf_For_ML/train/train/'\n",
    "file_name = \"test_img/test.png\"\n",
    "\n",
    "optimizer = Adam(0.001, 0.5) # lr and momentum\n",
    "\n",
    "noise_shape = (100,) #generator takes 1D array of size 100 as Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-06 15:37:58.476893: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/azadm/.local/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-10-06 15:37:58.476943: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-06 15:37:58.476986: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (azad-hplaptop15da0xxx): /proc/driver/nvidia/version does not exist\n",
      "2022-10-06 15:37:58.477435: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " generator_input (InputLayer  [(None, 100)]            0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 49152)             4964352   \n",
      "                                                                 \n",
      " Reshape_layer_1 (Reshape)   (None, 128, 128, 3)       0         \n",
      "                                                                 \n",
      " Transpose_layer_1 (Conv2DTr  (None, 128, 128, 256)    3328      \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " relu_1 (ReLU)               (None, 128, 128, 256)     0         \n",
      "                                                                 \n",
      " Batch_normalization_1 (Batc  (None, 128, 128, 256)    1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv_layer_1 (Conv2D)       (None, 128, 128, 256)     262400    \n",
      "                                                                 \n",
      " relu_2 (ReLU)               (None, 128, 128, 256)     0         \n",
      "                                                                 \n",
      " Batch_normalization_2 (Batc  (None, 128, 128, 256)    1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " Transpose_layer_2 (Conv2DTr  (None, 128, 128, 128)    131200    \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " relu_3 (ReLU)               (None, 128, 128, 128)     0         \n",
      "                                                                 \n",
      " Batch_normalization_3 (Batc  (None, 128, 128, 128)    512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv_layer_2 (Conv2D)       (None, 128, 128, 128)     65664     \n",
      "                                                                 \n",
      " relu_4 (ReLU)               (None, 128, 128, 128)     0         \n",
      "                                                                 \n",
      " Batch_normalization_4 (Batc  (None, 128, 128, 128)    512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " Transpose_layer_3 (Conv2DTr  (None, 128, 128, 64)     32832     \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " relu_5 (ReLU)               (None, 128, 128, 64)      0         \n",
      "                                                                 \n",
      " Batch_normalization_5 (Batc  (None, 128, 128, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv_layer_3 (Conv2D)       (None, 128, 128, 64)      16448     \n",
      "                                                                 \n",
      " relu_6 (ReLU)               (None, 128, 128, 64)      0         \n",
      "                                                                 \n",
      " Batch_normalization_6 (Batc  (None, 128, 128, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " Transpose_layer_4 (Conv2DTr  (None, 128, 128, 32)     8224      \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " relu_7 (ReLU)               (None, 128, 128, 32)      0         \n",
      "                                                                 \n",
      " Batch_normalization_7 (Batc  (None, 128, 128, 32)     128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv_layer_4 (Conv2D)       (None, 128, 128, 32)      4128      \n",
      "                                                                 \n",
      " relu_8 (ReLU)               (None, 128, 128, 32)      0         \n",
      "                                                                 \n",
      " Batch_normalization_8 (Batc  (None, 128, 128, 32)     128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " Transpose_layer_5 (Conv2DTr  (None, 128, 128, 16)     2064      \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " relu_9 (ReLU)               (None, 128, 128, 16)      0         \n",
      "                                                                 \n",
      " Batch_normalization_9 (Batc  (None, 128, 128, 16)     64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv_layer_5 (Conv2D)       (None, 128, 128, 16)      1040      \n",
      "                                                                 \n",
      " relu_10 (ReLU)              (None, 128, 128, 16)      0         \n",
      "                                                                 \n",
      " Batch_normalization_10 (Bat  (None, 128, 128, 16)     64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " Transpose_layer_6 (Conv2DTr  (None, 128, 128, 3)      195       \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " relu_11 (ReLU)              (None, 128, 128, 3)       0         \n",
      "                                                                 \n",
      " Batch_normalization_11 (Bat  (None, 128, 128, 3)      12        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv_layer_6 (Conv2D)       (None, 128, 128, 3)       39        \n",
      "                                                                 \n",
      " sigmoid_activation_layer (A  (None, 128, 128, 3)      0         \n",
      " ctivation)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,495,894\n",
      "Trainable params: 5,493,904\n",
      "Non-trainable params: 1,990\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=noise_shape, name='generator_input')\n",
    "L1 = Dense(np.prod(img_shape), name=\"Dense_1\")(input_layer)\n",
    "\n",
    "reshape_layer = Reshape(img_shape, name=\"Reshape_layer_1\")(L1)\n",
    "\n",
    "ct_1 = Conv2DTranspose(256, kernel_size=(2,2), \n",
    "                    padding=\"same\", \n",
    "                    strides=(1, 1), \n",
    "                    name=\"Transpose_layer_1\")(reshape_layer)\n",
    "ct_1 = ReLU(name=\"relu_1\")(ct_1)\n",
    "ct_1 = BatchNormalization(name=\"Batch_normalization_1\")(ct_1)              \n",
    "\n",
    "ct_1 = Conv2D(256, kernel_size=(2,2), \n",
    "                    padding=\"same\", \n",
    "                    strides=(1, 1), \n",
    "                    name=\"conv_layer_1\")(ct_1)\n",
    "ct_1 = ReLU(name=\"relu_2\")(ct_1)\n",
    "ct_1 = BatchNormalization(name=\"Batch_normalization_2\")(ct_1) \n",
    "\n",
    "\n",
    "ct_1 = Conv2DTranspose(128, kernel_size=(2,2), \n",
    "                    padding=\"same\", \n",
    "                    strides=(1, 1), \n",
    "                    name=\"Transpose_layer_2\")(ct_1)\n",
    "ct_1 = ReLU(name=\"relu_3\")(ct_1)\n",
    "ct_1 = BatchNormalization(name=\"Batch_normalization_3\")(ct_1)              \n",
    "\n",
    "ct_1 = Conv2D(128, kernel_size=(2,2), \n",
    "                    padding=\"same\", \n",
    "                    strides=(1, 1), \n",
    "                    name=\"conv_layer_2\")(ct_1)\n",
    "ct_1 = ReLU(name=\"relu_4\")(ct_1)\n",
    "ct_1 = BatchNormalization(name=\"Batch_normalization_4\")(ct_1) \n",
    "\n",
    "\n",
    "ct_1 = Conv2DTranspose(64, kernel_size=(2,2), \n",
    "                    padding=\"same\", \n",
    "                    strides=(1, 1), \n",
    "                    name=\"Transpose_layer_3\")(ct_1)\n",
    "ct_1 = ReLU(name=\"relu_5\")(ct_1)\n",
    "ct_1 = BatchNormalization(name=\"Batch_normalization_5\")(ct_1)              \n",
    "\n",
    "ct_1 = Conv2D(64, kernel_size=(2,2), \n",
    "                    padding=\"same\", \n",
    "                    strides=(1, 1), \n",
    "                    name=\"conv_layer_3\")(ct_1)\n",
    "ct_1 = ReLU(name=\"relu_6\")(ct_1)\n",
    "ct_1 = BatchNormalization(name=\"Batch_normalization_6\")(ct_1) \n",
    "\n",
    "\n",
    "ct_1 = Conv2DTranspose(32, kernel_size=(2,2), \n",
    "                    padding=\"same\", \n",
    "                    strides=(1, 1), \n",
    "                    name=\"Transpose_layer_4\")(ct_1)\n",
    "ct_1 = ReLU(name=\"relu_7\")(ct_1)\n",
    "ct_1 = BatchNormalization(name=\"Batch_normalization_7\")(ct_1)              \n",
    "\n",
    "ct_1 = Conv2D(32, kernel_size=(2,2), \n",
    "                    padding=\"same\", \n",
    "                    strides=(1, 1), \n",
    "                    name=\"conv_layer_4\")(ct_1)\n",
    "ct_1 = ReLU(name=\"relu_8\")(ct_1)\n",
    "ct_1 = BatchNormalization(name=\"Batch_normalization_8\")(ct_1) \n",
    "\n",
    "\n",
    "ct_1 = Conv2DTranspose(16, kernel_size=(2,2), \n",
    "                    padding=\"same\", \n",
    "                    strides=(1, 1), \n",
    "                    name=\"Transpose_layer_5\")(ct_1)\n",
    "ct_1 = ReLU(name=\"relu_9\")(ct_1)\n",
    "ct_1 = BatchNormalization(name=\"Batch_normalization_9\")(ct_1)              \n",
    "\n",
    "ct_1 = Conv2D(16, kernel_size=(2,2), \n",
    "                    padding=\"same\", \n",
    "                    strides=(1, 1), \n",
    "                    name=\"conv_layer_5\")(ct_1)\n",
    "ct_1 = ReLU(name=\"relu_10\")(ct_1)\n",
    "ct_1 = BatchNormalization(name=\"Batch_normalization_10\")(ct_1) \n",
    "\n",
    "\n",
    "ct_1 = Conv2DTranspose(3, kernel_size=(2,2), \n",
    "                    padding=\"same\", \n",
    "                    strides=(1, 1), \n",
    "                    name=\"Transpose_layer_6\")(ct_1)\n",
    "ct_1 = ReLU(name=\"relu_11\")(ct_1)\n",
    "ct_1 = BatchNormalization(name=\"Batch_normalization_11\")(ct_1)              \n",
    "\n",
    "ct_1 = Conv2D(3, kernel_size=(2,2), \n",
    "                    padding=\"same\", \n",
    "                    strides=(1, 1), \n",
    "                    name=\"conv_layer_6\")(ct_1)\n",
    "ct_1 = Activation(\"sigmoid\", name=\"sigmoid_activation_layer\")(ct_1)\n",
    "\n",
    "generator_model = Model(input_layer, ct_1)\n",
    "\n",
    "generator_model.summary()\n",
    "\n",
    "generator_model.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Discriminator_input_layer (  [(None, 128, 128, 3)]    0         \n",
      " InputLayer)                                                     \n",
      "                                                                 \n",
      " d_conv_1 (Conv2D)           (None, 64, 64, 256)       3328      \n",
      "                                                                 \n",
      " d_Batch_normalization_1 (Ba  (None, 64, 64, 256)      1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " d_conv_2 (Conv2D)           (None, 32, 32, 128)       131200    \n",
      "                                                                 \n",
      " d_Batch_normalization_2 (Ba  (None, 32, 32, 128)      512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " d_conv_3 (Conv2D)           (None, 16, 16, 64)        32832     \n",
      "                                                                 \n",
      " d_Batch_normalization_3 (Ba  (None, 16, 16, 64)       256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " d_conv_4 (Conv2D)           (None, 8, 8, 32)          8224      \n",
      "                                                                 \n",
      " d_Batch_normalization_4 (Ba  (None, 8, 8, 32)         128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " d_conv_5 (Conv2D)           (None, 4, 4, 16)          2064      \n",
      "                                                                 \n",
      " d_Batch_normalization_5 (Ba  (None, 4, 4, 16)         64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " Flatten_layer_1 (Flatten)   (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,681\n",
      "Trainable params: 244,689\n",
      "Non-trainable params: 992\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Build discriminator model\n",
    "\n",
    "input_layer  = Input(shape=(img_shape), name=\"Discriminator_input_layer\")\n",
    "\n",
    "c_1 = Conv2D(256, (2,2), \n",
    "            padding=\"same\", \n",
    "            strides=(2,2), \n",
    "            name=\"d_conv_1\",\n",
    "            activation='relu')(input_layer)\n",
    "c_1 = BatchNormalization(name=\"d_Batch_normalization_1\")(c_1)\n",
    "\n",
    "c_1 = Conv2D(128, (2,2), \n",
    "            padding=\"same\", \n",
    "            strides=(2,2), \n",
    "            name=\"d_conv_2\",\n",
    "            activation='relu')(c_1)\n",
    "c_1 = BatchNormalization(name=\"d_Batch_normalization_2\")(c_1)\n",
    "\n",
    "c_1 = Conv2D(64, (2,2), \n",
    "            padding=\"same\", \n",
    "            strides=(2,2), \n",
    "            name=\"d_conv_3\",\n",
    "            activation='relu')(c_1)\n",
    "c_1 = BatchNormalization(name=\"d_Batch_normalization_3\")(c_1)\n",
    "\n",
    "c_1 = Conv2D(32, (2,2), \n",
    "            padding=\"same\", \n",
    "            strides=(2,2), \n",
    "            name=\"d_conv_4\",\n",
    "            activation='relu')(c_1)\n",
    "c_1 = BatchNormalization(name=\"d_Batch_normalization_4\")(c_1)\n",
    "\n",
    "c_1 = Conv2D(16, (2,2), \n",
    "            padding=\"same\", \n",
    "            strides=(2,2), \n",
    "            name=\"d_conv_5\",\n",
    "            activation='relu')(c_1)\n",
    "c_1 = BatchNormalization(name=\"d_Batch_normalization_5\")(c_1)\n",
    "\n",
    "c_1 = Flatten(name=\"Flatten_layer_1\")(c_1)\n",
    "\n",
    "c_1 = Dense(256, activation='relu')(c_1)\n",
    "\n",
    "c_1 = Dense(1, activation='sigmoid')(c_1)\n",
    "\n",
    "discriminator_model = Model(input_layer, c_1)\n",
    "\n",
    "discriminator_model.summary()\n",
    "\n",
    "discriminator_model.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"GAN\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model (Functional)          (None, 128, 128, 3)       5495894   \n",
      "                                                                 \n",
      " model_3 (Functional)        (None, 1)                 245681    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,741,575\n",
      "Trainable params: 5,493,904\n",
      "Non-trainable params: 247,671\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def GAN(generator,discriminator):\n",
    "    discriminator.trainable = False\n",
    "    # Connet generator and discriminator\n",
    "    model = Sequential(name=\"GAN\")\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    return model\n",
    "\n",
    "gan = GAN(generator_model,discriminator_model)\n",
    "\n",
    "gan.summary()\n",
    "\n",
    "gan.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(img_folder):\n",
    "   \n",
    "    img_data_array=[]\n",
    "    class_name=[]\n",
    "   \n",
    "    for dir1 in os.listdir(img_folder):\n",
    "        for file in os.listdir(os.path.join(img_folder, dir1)):\n",
    "       \n",
    "            image_path= os.path.join(img_folder, dir1,  file)\n",
    "            image= cv2.imread( image_path, cv2.COLOR_BGR2RGB)\n",
    "            image=cv2.resize(image, (img_rows, img_cols),interpolation = cv2.INTER_AREA)\n",
    "            image=np.array(image)\n",
    "            image = image.astype('float32')\n",
    "            image /= 255 \n",
    "            img_data_array.append(image)\n",
    "            class_name.append(dir1)\n",
    "    return img_data_array, class_name\n",
    "\n",
    "def train(epochs, batch_size=128, save_interval=500):\n",
    "\n",
    "    x_train, class_name = create_dataset(img_dir)\n",
    "\n",
    "    # (x_train, _) , (_, _) = mnist.load_data()\n",
    "    # x_train = (x_train.astype(np.float32)-127.5)/127.5\n",
    "    #add channels dimention. As the input to our gen and discriminator. has a shape of 28x28x1\n",
    "    # x_train = np.expand_dims(x_train,axis=3)\n",
    "\n",
    "    x_train = np.array(x_train)\n",
    "    half_batch = int(batch_size/2)\n",
    "    # generator_model.load_weights('generator_model.h5')\n",
    "    # discriminator_model.load_weights('discriminator_model.h5')\n",
    "    # gan.load_weights('gan_model.h5')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # ------------------------------\n",
    "        # Train the discrimiunator model\n",
    "        # ------------------------------\n",
    "        # Select a random half batch of fake images\n",
    "\n",
    "        idx = np.random.randint(0,x_train.shape[0],half_batch)\n",
    "        imgs = x_train[idx]\n",
    "\n",
    "        noise = np.random.normal(0, 1, (half_batch,100))\n",
    "        # GEnerate a half batch of fake images\n",
    "        gen_images = generator_model.predict(noise)\n",
    "\n",
    "        #Train the discriminator on real and fake images, seperately\n",
    "        d_loss_real = discriminator_model.train_on_batch(imgs,np.ones((half_batch,1)))\n",
    "        d_loss_fake = discriminator_model.train_on_batch(gen_images,np.zeros((half_batch,1)))\n",
    "\n",
    "        d_loss = 0.5 * np.add(d_loss_real,d_loss_fake)\n",
    "\n",
    "        noise = np.random.normal(0,1,(batch_size,100))\n",
    "        #creates an array of all ones of size=batch size\n",
    "        valid_y = np.array([1] * batch_size)\n",
    "\n",
    "        g_loss = gan.train_on_batch(noise,valid_y)\n",
    "\n",
    "        print(\"%d [D loss: %f, acc: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0]))\n",
    "\n",
    "        if epoch % save_interval == 0:\n",
    "            save_imgs(epoch)\n",
    "\n",
    "def save_imgs(epoch):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0,1,(r * c, 100))\n",
    "    gen_imgs = generator_model.predict(noise)\n",
    "\n",
    "    # Rescale imgae 0 - 1\n",
    "    # gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,:])\n",
    "            axs[i,j].axis('off')\n",
    "            cnt+=1\n",
    "    fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "0 [D loss: 0.087755, acc: 100.00%] [G loss: 0.491752]\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1 [D loss: 0.136949, acc: 100.00%] [G loss: 0.436923]\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "2 [D loss: 0.048324, acc: 100.00%] [G loss: 0.392642]\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "3 [D loss: 0.101056, acc: 100.00%] [G loss: 0.377374]\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "4 [D loss: 0.092305, acc: 100.00%] [G loss: 0.344456]\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "5 [D loss: 0.048572, acc: 100.00%] [G loss: 0.308506]\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "6 [D loss: 0.043138, acc: 100.00%] [G loss: 0.267806]\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "7 [D loss: 0.039529, acc: 100.00%] [G loss: 0.236459]\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "8 [D loss: 0.051773, acc: 100.00%] [G loss: 0.212381]\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "9 [D loss: 0.042454, acc: 100.00%] [G loss: 0.189534]\n"
     ]
    }
   ],
   "source": [
    "train(epochs=10, batch_size=10, save_interval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_model.save('updated_generator_model.h5')\n",
    "discriminator_model.save('updated_discriminator_model.h5')\n",
    "gan.save('updated_gan_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
